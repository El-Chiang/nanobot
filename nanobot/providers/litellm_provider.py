"""LiteLLM provider implementation for multi-provider support."""

import json
import os
import re
from typing import Any
from uuid import uuid4

import litellm
from litellm import acompletion
from loguru import logger

from nanobot.providers.base import LLMProvider, LLMResponse, ToolCallRequest
from nanobot.providers.registry import find_by_model, find_gateway


class LiteLLMProvider(LLMProvider):
    """
    LLM provider using LiteLLM for multi-provider support.

    Supports OpenRouter, Anthropic, OpenAI, Gemini, MiniMax, and many other providers through
    a unified interface.  Provider-specific logic is driven by the registry
    (see providers/registry.py) — no if-elif chains needed here.
    """

    def __init__(
        self,
        api_key: str | None = None,
        api_base: str | None = None,
        default_model: str = "anthropic/claude-opus-4-5",
        extra_headers: dict[str, str] | None = None,
        provider_name: str | None = None,
        default_stream: bool = False,
    ):
        super().__init__(api_key, api_base)
        self.default_model = default_model
        self.extra_headers = extra_headers or {}
        self.default_stream = default_stream

        # Detect gateway / local deployment.
        # provider_name (from config key) is the primary signal;
        # api_key / api_base are fallback for auto-detection.
        self._gateway = find_gateway(provider_name, api_key, api_base)

        # Configure environment variables
        if api_key:
            self._setup_env(api_key, api_base, default_model)

        # Disable LiteLLM logging noise
        litellm.suppress_debug_info = True
        # Drop unsupported parameters for providers (e.g., gpt-5 rejects some params)
        litellm.drop_params = True

    def _setup_env(self, api_key: str, api_base: str | None, model: str) -> None:
        """Set environment variables based on detected provider."""
        spec = self._gateway or find_by_model(model)
        if not spec:
            return

        # Gateway/local overrides existing env; standard provider doesn't
        if self._gateway:
            os.environ[spec.env_key] = api_key
        else:
            os.environ.setdefault(spec.env_key, api_key)

        # Resolve env_extras placeholders:
        #   {api_key}  → user's API key
        #   {api_base} → user's api_base, falling back to spec.default_api_base
        effective_base = api_base or spec.default_api_base
        for env_name, env_val in spec.env_extras:
            resolved = env_val.replace("{api_key}", api_key)
            resolved = resolved.replace("{api_base}", effective_base)
            os.environ.setdefault(env_name, resolved)

    def _resolve_model(self, model: str) -> str:
        """Resolve model name by applying provider/gateway prefixes."""
        if self._gateway:
            # Gateway mode: apply gateway prefix, skip provider-specific prefixes
            prefix = self._gateway.litellm_prefix
            if self._gateway.strip_model_prefix:
                model = model.split("/")[-1]
            if prefix and not model.startswith(f"{prefix}/"):
                model = f"{prefix}/{model}"
            return model

        # Standard mode: auto-prefix for known providers
        spec = find_by_model(model)
        if spec and spec.litellm_prefix:
            if not any(model.startswith(s) for s in spec.skip_prefixes):
                model = f"{spec.litellm_prefix}/{model}"

        return model

    def _apply_model_overrides(self, model: str, kwargs: dict[str, Any]) -> None:
        """Apply model-specific parameter overrides from the registry."""
        model_lower = model.lower()
        spec = find_by_model(model)
        if spec:
            for pattern, overrides in spec.model_overrides:
                if pattern in model_lower:
                    kwargs.update(overrides)
                    return

    def _preview_text(self, text: str | None, limit: int = 240) -> str:
        """Build a single-line truncated preview for logs."""
        if text is None:
            return "<none>"
        compact = text.replace("\n", "\\n")
        if compact.strip() == "":
            return "<blank>"
        if len(compact) > limit:
            return f"{compact[:limit]}...(truncated)"
        return compact

    @staticmethod
    def _is_custom_gemini_proxy(model: str, api_base: str | None) -> bool:
        """Detect Gemini calls routed through non-official proxy endpoints."""
        if "gemini/" not in model.lower():
            return False
        if not api_base:
            return False
        base = api_base.lower()
        return "generativelanguage.googleapis.com" not in base

    def _prepare_messages_for_gemini_stream_proxy(
        self, messages: list[dict[str, Any]], model: str, api_base: str | None
    ) -> list[dict[str, Any]]:
        """
        Clean history for stream fallback on Gemini-compatible proxies.

        Some proxies reject OpenAI-style tool history blocks in Gemini stream mode.
        Keep only text conversation turns and remove tool metadata from history.
        """
        if not self._is_custom_gemini_proxy(model, api_base):
            return messages

        normalized: list[dict[str, Any]] = []
        for msg in messages:
            role = msg.get("role")
            if role not in {"system", "user", "assistant"}:
                # Drop tool/function messages from history.
                continue

            entry = dict(msg)
            entry.pop("tool_calls", None)
            entry.pop("tool_call_id", None)
            entry.pop("name", None)

            # Skip empty assistant placeholders (commonly tool-call wrappers).
            if role == "assistant":
                content = entry.get("content")
                if content is None:
                    continue
                if isinstance(content, str) and content.strip() == "":
                    continue

            normalized.append(entry)

        return normalized or messages

    def _log_response_summary(self, response: LLMResponse) -> None:
        """Log a compact summary of the model response."""
        content_preview = self._preview_text(response.content)
        if response.has_tool_calls:
            names = ", ".join(tc.name for tc in response.tool_calls[:5])
            if len(response.tool_calls) > 5:
                names = f"{names}, ..."
            logger.debug(
                "LLM Response: mode=function_call, "
                f"finish_reason={response.finish_reason}, "
                f"tool_calls={len(response.tool_calls)}, "
                f"tool_names=[{names}], "
                f"content={content_preview}"
            )
        else:
            logger.debug(
                "LLM Response: mode=text, "
                f"finish_reason={response.finish_reason}, "
                f"content={content_preview}"
            )

    @staticmethod
    def _format_exception(exc: Exception) -> str:
        """Return a stable, searchable exception summary."""
        name = type(exc).__name__
        msg = str(exc).strip()
        return f"{name}: {msg}" if msg else name

    async def chat(
        self,
        messages: list[dict[str, Any]],
        tools: list[dict[str, Any]] | None = None,
        model: str | None = None,
        max_tokens: int = 4096,
        temperature: float = 0.7,
        thinking: str | None = None,
        thinking_budget: int = 10000,
        effort: str | None = None,
    ) -> LLMResponse:
        """
        Send a chat completion request via LiteLLM.

        Args:
            messages: List of message dicts with 'role' and 'content'.
            tools: Optional list of tool definitions in OpenAI format.
            model: Model identifier (e.g., 'anthropic/claude-sonnet-4-5').
            max_tokens: Maximum tokens in response.
            temperature: Sampling temperature.

        Returns:
            LLMResponse with content and/or tool calls.
        """
        model = self._resolve_model(model or self.default_model)
        kwargs: dict[str, Any] = {
            "model": model,
            "messages": messages,
            "max_tokens": max_tokens,
            "temperature": temperature,
        }

        # Apply model-specific overrides (e.g. kimi-k2.5 temperature)
        self._apply_model_overrides(model, kwargs)

        # Pass api_key directly — more reliable than env vars alone
        if self.api_key:
            kwargs["api_key"] = self.api_key

        # Pass api_base for custom endpoints
        if self.api_base:
            kwargs["api_base"] = self.api_base

        # Pass api_key explicitly to avoid env var interference
        if self.api_key:
            kwargs["api_key"] = self.api_key

        # Pass extra headers (e.g. APP-Code for AiHubMix)
        if self.extra_headers:
            kwargs["extra_headers"] = self.extra_headers

        if tools:
            kwargs["tools"] = tools
            kwargs["tool_choice"] = "auto"

        # Extended thinking (Anthropic models)
        if thinking == "enabled":
            kwargs["thinking"] = {"type": "enabled", "budget_tokens": thinking_budget}
        elif thinking == "adaptive":
            kwargs["thinking"] = {"type": "adaptive"}

        # Effort control via output_config (Anthropic native)
        if effort is not None:
            kwargs["output_config"] = {"effort": effort}

        try:
            logger.debug(
                f"LLM Request: model={kwargs.get('model')}, api_base={kwargs.get('api_base')}, "
                f"default_stream={self.default_stream}"
            )
            if self.default_stream:
                stream_kwargs = self._prepare_stream_kwargs(kwargs)
                return await self._stream_chat(stream_kwargs)

            non_stream_kwargs = self._prepare_non_stream_kwargs(kwargs)
            response = await acompletion(**non_stream_kwargs)
            parsed = self._parse_response(response)
            self._log_response_summary(parsed)
            return parsed
        except Exception as e:
            # 根据默认模式互相兜底
            if self.default_stream:
                logger.warning(
                    "Stream call failed, falling back to non-stream: "
                    f"{self._format_exception(e)}"
                )
            else:
                logger.warning(
                    "Non-stream call failed, falling back to stream: "
                    f"{self._format_exception(e)}"
                )
            try:
                if self.default_stream:
                    non_stream_kwargs = self._prepare_non_stream_kwargs(kwargs)
                    response = await acompletion(**non_stream_kwargs)
                    parsed = self._parse_response(response)
                    self._log_response_summary(parsed)
                    return parsed

                stream_kwargs = self._prepare_stream_kwargs(kwargs)
                return await self._stream_chat(stream_kwargs)
            except Exception as fallback_e:
                err = self._format_exception(fallback_e)
                logger.warning(f"LLM Response: mode=error, error={err}")
                return LLMResponse(
                    content=f"Error calling LLM: {err}",
                    finish_reason="error",
                )

    async def _stream_chat(self, kwargs: dict[str, Any]) -> LLMResponse:
        """Stream 调用并拼装完整响应"""
        kwargs["stream"] = True

        content_parts: list[str] = []
        tool_calls_map: dict[int, dict] = {}  # index -> {id, name, arguments}
        finish_reason = "stop"
        usage: dict[str, Any] = {}
        reasoning_parts: list[str] = []

        async for chunk in await acompletion(**kwargs):
            choice = chunk.choices[0] if chunk.choices else None
            if not choice:
                continue

            delta = choice.delta

            # 收集 content
            if delta.content:
                content_parts.append(delta.content)

            # 收集 reasoning_content（如果有）
            if hasattr(delta, "reasoning_content") and delta.reasoning_content:
                reasoning_parts.append(delta.reasoning_content)

            # 收集 tool_calls（分片合并）
            if hasattr(delta, "tool_calls") and delta.tool_calls:
                for tc in delta.tool_calls:
                    idx = tc.index
                    if idx not in tool_calls_map:
                        tool_calls_map[idx] = {
                            "id": tc.id or "",
                            "name": tc.function.name if tc.function else "",
                            "arguments": "",
                        }
                    if tc.id:
                        tool_calls_map[idx]["id"] = tc.id
                    if tc.function:
                        if tc.function.name:
                            tool_calls_map[idx]["name"] = tc.function.name
                        if tc.function.arguments:
                            tool_calls_map[idx]["arguments"] += tc.function.arguments

            # 获取 finish_reason
            if choice.finish_reason:
                finish_reason = choice.finish_reason

            # 获取 usage（通常在最后一个 chunk）
            if hasattr(chunk, "usage") and chunk.usage:
                usage = {
                    "prompt_tokens": chunk.usage.prompt_tokens,
                    "completion_tokens": chunk.usage.completion_tokens,
                    "total_tokens": chunk.usage.total_tokens,
                }

        # 构建 tool_calls 列表
        tool_calls = []
        for idx in sorted(tool_calls_map.keys()):
            tc_data = tool_calls_map[idx]
            args = tc_data["arguments"]
            if isinstance(args, str):
                try:
                    args = json.loads(args)
                except json.JSONDecodeError:
                    args = {"raw": args}
            tool_calls.append(
                ToolCallRequest(
                    id=tc_data["id"],
                    name=tc_data["name"],
                    arguments=args,
                )
            )

        content = "".join(content_parts) if content_parts else None
        reasoning_content = "".join(reasoning_parts) if reasoning_parts else None

        # Stream some proxy/model variants emit textual pseudo tool calls:
        #   [tool_call] message({...})
        # Coerce them into structured tool calls (stream-only fallback).
        if not tool_calls:
            content, parsed_tool_calls = self._coerce_stream_text_tool_calls(content)
            if parsed_tool_calls:
                tool_calls = parsed_tool_calls
                logger.warning(
                    "Stream text tool_call marker detected; coerced "
                    f"{len(parsed_tool_calls)} tool call(s)."
                )

        response = LLMResponse(
            content=content,
            tool_calls=tool_calls,
            finish_reason=finish_reason,
            usage=usage,
            reasoning_content=reasoning_content,
        )

        self._log_response_summary(response)
        return response

    def _prepare_stream_kwargs(self, kwargs: dict[str, Any]) -> dict[str, Any]:
        """Clone kwargs and normalize messages for stream-sensitive proxies."""
        stream_kwargs = dict(kwargs)
        stream_kwargs["messages"] = self._prepare_messages_for_gemini_stream_proxy(
            kwargs["messages"], kwargs["model"], kwargs.get("api_base")
        )
        return stream_kwargs

    def _prepare_non_stream_kwargs(self, kwargs: dict[str, Any]) -> dict[str, Any]:
        """Clone kwargs and normalize messages for non-stream Gemini proxy calls."""
        non_stream_kwargs = dict(kwargs)
        non_stream_kwargs["messages"] = self._prepare_messages_for_gemini_stream_proxy(
            kwargs["messages"], kwargs["model"], kwargs.get("api_base")
        )
        return non_stream_kwargs

    def _coerce_stream_text_tool_calls(
        self, content: str | None
    ) -> tuple[str | None, list[ToolCallRequest]]:
        """Parse textual [tool_call] markers from stream text into ToolCallRequest."""
        if not content or "[tool_call]" not in content:
            return content, []

        token = "[tool_call]"
        decoder = json.JSONDecoder()
        calls: list[ToolCallRequest] = []
        spans: list[tuple[int, int]] = []
        pos = 0

        while True:
            start = content.find(token, pos)
            if start < 0:
                break

            idx = start + len(token)
            header = re.match(r"\s*([A-Za-z_]\w*)\s*\(", content[idx:])
            if not header:
                pos = start + len(token)
                continue

            name = header.group(1)
            idx += header.end()

            tail = content[idx:].lstrip()
            idx += len(content[idx:]) - len(tail)
            if not tail.startswith("{"):
                pos = start + len(token)
                continue

            try:
                args_obj, consumed = decoder.raw_decode(tail)
            except json.JSONDecodeError:
                pos = start + len(token)
                continue

            idx += consumed
            trailing = content[idx:].lstrip()
            idx += len(content[idx:]) - len(trailing)
            if idx >= len(content) or content[idx] != ")":
                pos = start + len(token)
                continue

            end = idx + 1
            spans.append((start, end))
            calls.append(
                ToolCallRequest(
                    id=f"text_toolcall_{uuid4().hex[:12]}",
                    name=name,
                    arguments=args_obj if isinstance(args_obj, dict) else {"raw": args_obj},
                )
            )
            pos = end

        if not calls:
            return content, []

        parts: list[str] = []
        cursor = 0
        for start, end in spans:
            parts.append(content[cursor:start])
            cursor = end
        parts.append(content[cursor:])

        cleaned = "".join(parts)
        cleaned = re.sub(r"\n{3,}", "\n\n", cleaned).strip()
        return (cleaned or None), calls

    def _parse_response(self, response: Any) -> LLMResponse:
        """Parse LiteLLM response into our standard format."""
        choice = response.choices[0]
        message = choice.message

        tool_calls = []
        if hasattr(message, "tool_calls") and message.tool_calls:
            for tc in message.tool_calls:
                # Parse arguments from JSON string if needed
                args = tc.function.arguments
                if isinstance(args, str):
                    try:
                        args = json.loads(args)
                    except json.JSONDecodeError:
                        args = {"raw": args}

                tool_calls.append(
                    ToolCallRequest(
                        id=tc.id,
                        name=tc.function.name,
                        arguments=args,
                    )
                )

        usage = {}
        if hasattr(response, "usage") and response.usage:
            usage = {
                "prompt_tokens": response.usage.prompt_tokens,
                "completion_tokens": response.usage.completion_tokens,
                "total_tokens": response.usage.total_tokens,
            }

        reasoning_content = getattr(message, "reasoning_content", None)

        return LLMResponse(
            content=message.content,
            tool_calls=tool_calls,
            finish_reason=choice.finish_reason or "stop",
            usage=usage,
            reasoning_content=reasoning_content,
        )

    def get_default_model(self) -> str:
        """Get the default model."""
        return self.default_model
